{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Go Benchmark Results Analyzer\n\nThis notebook analyzes Go benchmark output files, extracting performance metrics and generating visualizations.\n\n## Features\n- **Automatic folder selection**: Automatically analyzes the latest benchmark run if no folder is specified\n- Parse Go benchmark output files\n- Extract performance metrics (ns/op, MB/s, B/op, allocs/op)\n- Compare SIMD vs Fallback performance\n- Visualize throughput, memory usage, and scaling behavior\n- Support for custom metrics in benchmark output\n- CPU profile analysis (.prof files)\n- Export results to CSV for further analysis\n\n## Usage\nBy default, this notebook automatically selects the most recent `run_*_analysis` folder based on timestamp. To analyze a specific folder, set the `RUN_FOLDER` parameter in the configuration cell below."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style for better-looking plots\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Benchmark Files\n",
    "\n",
    "Find all benchmark result files in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration: Select files for this analysis session\n# Group related files together for a single investigation\n\n# ============================================================================\n# BENCHMARK FILE SELECTION\n# ============================================================================\n# PARAMETER: Set RUN_FOLDER to specify which analysis folder to use\n# If set to None, the latest timestamped folder will be automatically selected\n\nRUN_FOLDER = None  # Set to None to auto-select latest, or specify path like \"results/run_20251031_111520_analysis\"\n\n# ============================================================================\n# AUTO-SELECT LATEST FOLDER IF NOT SPECIFIED\n# ============================================================================\ndef find_latest_analysis_folder(results_dir=\"results\"):\n    \"\"\"\n    Find the most recent analysis folder based on timestamp in folder name.\n\n    Returns:\n        Path to the latest folder, or None if no folders found\n    \"\"\"\n    import glob\n    import re\n\n    # Find all run_* folders\n    pattern = os.path.join(results_dir, \"run_*_analysis\")\n    folders = glob.glob(pattern)\n\n    if not folders:\n        return None\n\n    # Extract timestamps and sort\n    folder_times = []\n    for folder in folders:\n        basename = os.path.basename(folder)\n        # Extract timestamp: run_YYYYMMDD_HHMMSS_analysis\n        match = re.search(r'run_(\\d{8}_\\d{6})_analysis', basename)\n        if match:\n            timestamp = match.group(1)\n            folder_times.append((timestamp, folder))\n\n    if not folder_times:\n        return None\n\n    # Sort by timestamp (descending) and return the latest\n    folder_times.sort(reverse=True)\n    return folder_times[0][1]\n\nif RUN_FOLDER is None:\n    RUN_FOLDER = find_latest_analysis_folder()\n    if RUN_FOLDER:\n        print(\"=\" * 80)\n        print(\"AUTO-SELECTED LATEST ANALYSIS FOLDER\")\n        print(\"=\" * 80)\n        print(f\"Using: {RUN_FOLDER}\")\n        print(\"\\nTo analyze a different folder, set RUN_FOLDER in this cell to:\")\n        print('  RUN_FOLDER = r\"results\\\\run_YYYYMMDD_HHMMSS_analysis\"')\n        print(\"=\" * 80)\n    else:\n        print(\"=\" * 80)\n        print(\"ERROR: No analysis folders found\")\n        print(\"=\" * 80)\n        print(\"No 'run_*_analysis' folders found in results/\")\n        print(\"Please run the benchmark script first:\")\n        print(\"  bash scripts/benchmark.sh\")\n        print(\"=\" * 80)\n        raise FileNotFoundError(\"No analysis folders found in results/\")\n\n# Set up file paths based on selected folder\nBENCHMARK_FILE = os.path.join(RUN_FOLDER, \"benchmark_full_suite.txt\")\nPROFILE_FILE = os.path.join(RUN_FOLDER, \"cpu_profile.prof\")\nPROFILE_TREE_FILE = os.path.join(RUN_FOLDER, \"profile_tree.txt\")\n\n# ============================================================================\n# ANALYSIS CONFIGURATION\n# ============================================================================\n# Extract analysis name from folder\nANALYSIS_NAME = os.path.basename(RUN_FOLDER).replace('_analysis', '')\n\n# ============================================================================\n# VALIDATION\n# ============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"ANALYSIS CONFIGURATION\")\nprint(\"=\" * 80)\nprint(f\"\\nAnalysis Folder: {RUN_FOLDER}\")\nprint(f\"Analysis Name: {ANALYSIS_NAME}\")\n\nif not os.path.exists(BENCHMARK_FILE):\n    print(f\"\\n✗ ERROR: Benchmark file not found: {BENCHMARK_FILE}\")\n    print(\"Please run the benchmark script first:\")\n    print(\"  bash scripts/benchmark.sh\")\n    raise FileNotFoundError(f\"Benchmark file not found: {BENCHMARK_FILE}\")\nelse:\n    print(f\"\\n✓ Benchmark file: {os.path.basename(BENCHMARK_FILE)}\")\n    print(f\"  Size: {os.path.getsize(BENCHMARK_FILE):,} bytes\")\n\nif PROFILE_FILE:\n    if not os.path.exists(PROFILE_FILE):\n        print(f\"\\n○ CPU Profile: Not found (analysis will be skipped)\")\n        PROFILE_FILE = None\n    else:\n        print(f\"\\n✓ CPU Profile: {os.path.basename(PROFILE_FILE)}\")\n        print(f\"  Size: {os.path.getsize(PROFILE_FILE):,} bytes\")\nelse:\n    print(f\"\\n○ CPU Profile: None\")\n\nif PROFILE_TREE_FILE:\n    if not os.path.exists(PROFILE_TREE_FILE):\n        print(f\"\\n○ Profile Tree: Not found (analysis will be skipped)\")\n        PROFILE_TREE_FILE = None\n    else:\n        print(f\"\\n✓ Profile Tree: {os.path.basename(PROFILE_TREE_FILE)}\")\n        print(f\"  Size: {os.path.getsize(PROFILE_TREE_FILE):,} bytes\")\nelse:\n    print(f\"\\n○ Profile Tree: None\")\n\n# Set up output directory - output goes inside the run folder\noutput_dir = RUN_FOLDER\nprint(f\"\\n✓ Output directory: {output_dir}\")\n\nos.makedirs(os.path.join(output_dir, 'graphs'), exist_ok=True)\nos.makedirs(os.path.join(output_dir, 'data'), exist_ok=True)\n\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Benchmark File\n",
    "\n",
    "Extract all benchmark results and custom metrics from the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_benchmark_file(filepath):\n",
    "    \"\"\"\n",
    "    Parse Go benchmark output file and extract all metrics.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with columns: name, iterations, ns_op, custom_metrics, bytes_op, allocs_op\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Regex pattern for benchmark lines\n",
    "    # Format: BenchmarkName-N   iterations   ns/op   [custom metrics]   B/op   allocs/op\n",
    "    pattern = r'^(Benchmark\\S+)\\s+(\\d+)\\s+(\\d+\\.?\\d*)\\s+ns/op(.*)$'\n",
    "    \n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            match = re.match(pattern, line)\n",
    "            \n",
    "            if match:\n",
    "                name = match.group(1)\n",
    "                iterations = int(match.group(2))\n",
    "                ns_op = float(match.group(3))\n",
    "                rest = match.group(4).strip()\n",
    "                \n",
    "                # Parse custom metrics and memory stats\n",
    "                custom_metrics = {}\n",
    "                bytes_op = 0\n",
    "                allocs_op = 0\n",
    "                \n",
    "                # Extract all key-value pairs (e.g., \"17.76 MB/s\", \"144 B/op\")\n",
    "                metric_pattern = r'(\\d+\\.?\\d*)\\s+([A-Za-z_/]+)'\n",
    "                for match in re.finditer(metric_pattern, rest):\n",
    "                    value = float(match.group(1))\n",
    "                    unit = match.group(2)\n",
    "                    \n",
    "                    if unit == 'B/op':\n",
    "                        bytes_op = value\n",
    "                    elif unit == 'allocs/op':\n",
    "                        allocs_op = value\n",
    "                    else:\n",
    "                        # Store custom metrics (MB/s, cache_lines, etc.)\n",
    "                        custom_metrics[unit] = value\n",
    "                \n",
    "                results.append({\n",
    "                    'name': name,\n",
    "                    'iterations': iterations,\n",
    "                    'ns_op': ns_op,\n",
    "                    'bytes_op': bytes_op,\n",
    "                    'allocs_op': allocs_op,\n",
    "                    **custom_metrics\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Add computed columns\n",
    "    if not df.empty:\n",
    "        # Extract benchmark category and variant\n",
    "        df['category'] = df['name'].str.extract(r'Benchmark([^/]+)')[0]\n",
    "        df['variant'] = df['name'].str.extract(r'/([^-]+)-\\d+$')[0]\n",
    "        \n",
    "        # Extract size if present\n",
    "        df['size'] = df['name'].str.extract(r'Size_(\\d+)')[0]\n",
    "        df['size'] = pd.to_numeric(df['size'], errors='coerce')\n",
    "        \n",
    "        # Compute throughput (ops/sec)\n",
    "        df['ops_per_sec'] = 1e9 / df['ns_op']\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Parse the benchmark file\n",
    "df = parse_benchmark_file(BENCHMARK_FILE)\n",
    "\n",
    "print(f\"\\nParsed {len(df)} benchmark results\")\n",
    "print(f\"\\nColumns: {', '.join(df.columns)}\")\n",
    "print(f\"\\nBenchmark categories: {', '.join(df['category'].unique())}\")\n",
    "\n",
    "# Display first few rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nTotal benchmarks: {len(df)}\")\n",
    "print(f\"Categories: {len(df['category'].unique())}\")\n",
    "print(f\"\\nPerformance range:\")\n",
    "print(f\"  Fastest: {df['ns_op'].min():.2f} ns/op ({df.loc[df['ns_op'].idxmin(), 'name']})\")\n",
    "print(f\"  Slowest: {df['ns_op'].max():.2f} ns/op ({df.loc[df['ns_op'].idxmax(), 'name']})\")\n",
    "\n",
    "if 'MB/s' in df.columns:\n",
    "    print(f\"\\nThroughput range:\")\n",
    "    print(f\"  Highest: {df['MB/s'].max():.2f} MB/s\")\n",
    "    print(f\"  Lowest: {df['MB/s'].min():.2f} MB/s\")\n",
    "\n",
    "print(f\"\\nMemory allocation:\")\n",
    "print(f\"  Max bytes/op: {df['bytes_op'].max():.0f} B/op\")\n",
    "print(f\"  Max allocs/op: {df['allocs_op'].max():.0f} allocs/op\")\n",
    "print(f\"  Zero-allocation benchmarks: {len(df[df['allocs_op'] == 0])} / {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIMD vs Fallback Comparison\n",
    "\n",
    "Analyze performance differences between SIMD and fallback implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter SIMD comparison benchmarks\n",
    "simd_df = df[df['name'].str.contains('SIMDvsScalar', na=False)].copy()\n",
    "\n",
    "if not simd_df.empty:\n",
    "    # Extract operation type and implementation\n",
    "    simd_df['operation'] = simd_df['name'].str.extract(r'/(\\w+)_Size')[0]\n",
    "    simd_df['implementation'] = simd_df['name'].str.extract(r'/(SIMD|Fallback)-')[0]\n",
    "    \n",
    "    # Pivot for comparison\n",
    "    comparison = simd_df.pivot_table(\n",
    "        index=['size', 'operation'],\n",
    "        columns='implementation',\n",
    "        values='ns_op'\n",
    "    ).reset_index()\n",
    "    \n",
    "    if 'SIMD' in comparison.columns and 'Fallback' in comparison.columns:\n",
    "        comparison['speedup'] = comparison['Fallback'] / comparison['SIMD']\n",
    "        comparison['improvement_pct'] = (comparison['speedup'] - 1) * 100\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"SIMD PERFORMANCE ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"\\nAverage SIMD speedup: {comparison['speedup'].mean():.2f}x\")\n",
    "        print(f\"Best SIMD speedup: {comparison['speedup'].max():.2f}x (size {comparison.loc[comparison['speedup'].idxmax(), 'size']})\")\n",
    "        print(f\"Worst SIMD speedup: {comparison['speedup'].min():.2f}x (size {comparison.loc[comparison['speedup'].idxmin(), 'size']})\")\n",
    "        \n",
    "        print(\"\\nSpeedup by operation:\")\n",
    "        op_speedup = comparison.groupby('operation')['speedup'].mean().sort_values(ascending=False)\n",
    "        for op, speedup in op_speedup.items():\n",
    "            print(f\"  {op:15s}: {speedup:.2f}x\")\n",
    "        \n",
    "        display(comparison.sort_values('speedup', ascending=False))\n",
    "else:\n",
    "    print(\"No SIMD comparison benchmarks found in this file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: SIMD Speedup by Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not simd_df.empty and 'speedup' in comparison.columns:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Speedup by size for each operation\n",
    "    for operation in comparison['operation'].unique():\n",
    "        op_data = comparison[comparison['operation'] == operation]\n",
    "        ax1.plot(op_data['size'], op_data['speedup'], marker='o', label=operation, linewidth=2)\n",
    "    \n",
    "    ax1.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='No speedup')\n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_xlabel('Data Size (bytes)', fontsize=12)\n",
    "    ax1.set_ylabel('Speedup (Fallback / SIMD)', fontsize=12)\n",
    "    ax1.set_title('SIMD Speedup vs Data Size', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Speedup by operation (bar chart)\n",
    "    op_speedup = comparison.groupby('operation')['speedup'].mean().sort_values(ascending=True)\n",
    "    colors = ['green' if x > 1 else 'red' for x in op_speedup.values]\n",
    "    op_speedup.plot(kind='barh', ax=ax2, color=colors, alpha=0.7)\n",
    "    ax2.axvline(x=1, color='red', linestyle='--', alpha=0.5)\n",
    "    ax2.set_xlabel('Average Speedup (x)', fontsize=12)\n",
    "    ax2.set_title('Average SIMD Speedup by Operation', fontsize=14, fontweight='bold')\n",
    "    ax2.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    graph_path = os.path.join(output_dir, 'graphs', 'simd_speedup_analysis.png')\n",
    "    plt.savefig(graph_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Graph saved: {graph_path}\")\n",
    "    \n",
    "    # Also save as SVG for high-quality reports\n",
    "    svg_path = os.path.join(output_dir, 'graphs', 'simd_speedup_analysis.svg')\n",
    "    plt.savefig(svg_path, format='svg', bbox_inches='tight')\n",
    "    print(f\"✓ Graph saved (SVG): {svg_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Cannot plot SIMD comparison - no data available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Performance Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter benchmarks with size information\n",
    "sized_df = df[df['size'].notna()].copy()\n",
    "\n",
    "if not sized_df.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: ns/op vs size\n",
    "    for category in sized_df['category'].unique()[:5]:  # Limit to 5 categories\n",
    "        cat_data = sized_df[sized_df['category'] == category]\n",
    "        ax1.plot(cat_data['size'], cat_data['ns_op'], marker='o', label=category, linewidth=2)\n",
    "    \n",
    "    ax1.set_xscale('log')\n",
    "    ax1.set_yscale('log')\n",
    "    ax1.set_xlabel('Size', fontsize=12)\n",
    "    ax1.set_ylabel('Time (ns/op)', fontsize=12)\n",
    "    ax1.set_title('Performance Scaling', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Throughput\n",
    "    for category in sized_df['category'].unique()[:5]:\n",
    "        cat_data = sized_df[sized_df['category'] == category]\n",
    "        ax2.plot(cat_data['size'], cat_data['ops_per_sec'], marker='o', label=category, linewidth=2)\n",
    "    \n",
    "    ax2.set_xscale('log')\n",
    "    ax2.set_yscale('log')\n",
    "    ax2.set_xlabel('Size', fontsize=12)\n",
    "    ax2.set_ylabel('Throughput (ops/sec)', fontsize=12)\n",
    "    ax2.set_title('Throughput Scaling', fontsize=14, fontweight='bold')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    graph_path = os.path.join(output_dir, 'graphs', 'performance_scaling.png')\n",
    "    plt.savefig(graph_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Graph saved: {graph_path}\")\n",
    "    \n",
    "    svg_path = os.path.join(output_dir, 'graphs', 'performance_scaling.svg')\n",
    "    plt.savefig(svg_path, format='svg', bbox_inches='tight')\n",
    "    print(f\"✓ Graph saved (SVG): {svg_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No size-based benchmarks found for scaling analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization: Memory Allocation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter benchmarks with memory allocations\n",
    "mem_df = df[df['bytes_op'] > 0].copy()\n",
    "\n",
    "if not mem_df.empty:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Memory allocation by category (top 10)\n",
    "    mem_by_cat = mem_df.groupby('category')['bytes_op'].mean().sort_values(ascending=False).head(10)\n",
    "    mem_by_cat.plot(kind='barh', ax=ax1, color='steelblue', alpha=0.7)\n",
    "    ax1.set_xlabel('Average Bytes/op', fontsize=12)\n",
    "    ax1.set_title('Memory Allocation by Category', fontsize=14, fontweight='bold')\n",
    "    ax1.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Plot 2: Allocations vs Performance\n",
    "    ax2.scatter(mem_df['allocs_op'], mem_df['ns_op'], alpha=0.6, s=100)\n",
    "    ax2.set_xlabel('Allocations/op', fontsize=12)\n",
    "    ax2.set_ylabel('Time (ns/op)', fontsize=12)\n",
    "    ax2.set_title('Allocations vs Performance', fontsize=14, fontweight='bold')\n",
    "    ax2.set_yscale('log')\n",
    "    if mem_df['allocs_op'].max() > 10:\n",
    "        ax2.set_xscale('log')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    graph_path = os.path.join(output_dir, 'graphs', 'memory_allocation_analysis.png')\n",
    "    plt.savefig(graph_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"✓ Graph saved: {graph_path}\")\n",
    "    \n",
    "    svg_path = os.path.join(output_dir, 'graphs', 'memory_allocation_analysis.svg')\n",
    "    plt.savefig(svg_path, format='svg', bbox_inches='tight')\n",
    "    print(f\"✓ Graph saved (SVG): {svg_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Zero-allocation performance\n",
    "    zero_alloc = df[df['allocs_op'] == 0]\n",
    "    print(f\"\\nZero-allocation benchmarks: {len(zero_alloc)} / {len(df)}\")\n",
    "    print(f\"Average performance (zero-alloc): {zero_alloc['ns_op'].mean():.2f} ns/op\")\n",
    "    print(f\"Average performance (with-alloc): {mem_df['ns_op'].mean():.2f} ns/op\")\n",
    "else:\n",
    "    print(\"No memory allocation data found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hybrid Mode Analysis\n",
    "\n",
    "Analyze array vs map mode performance if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_df = df[df['name'].str.contains('HybridModes', na=False)].copy()\n",
    "\n",
    "if not hybrid_df.empty:\n",
    "    # Extract mode (Array/Map) and operation\n",
    "    hybrid_df['mode'] = hybrid_df['name'].str.extract(r'(Array|Map)')[0]\n",
    "    hybrid_df['operation'] = hybrid_df['name'].str.extract(r'(Add|Contains)')[0]\n",
    "    \n",
    "    # Extract size information from name (e.g., \"Small_1K\", \"Large_1M\")\n",
    "    size_map = {'1K': 1000, '10K': 10000, '100K': 100000, '1M': 1000000, '10M': 10000000}\n",
    "    hybrid_df['size_label'] = hybrid_df['name'].str.extract(r'(Small|Medium|Large)_(\\d+[KM])')[1]\n",
    "    hybrid_df['size_numeric'] = hybrid_df['size_label'].map(size_map)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"HYBRID MODE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Compare Array vs Map performance\n",
    "    if 'MB/s' in hybrid_df.columns:\n",
    "        print(\"\\nThroughput comparison:\")\n",
    "        mode_perf = hybrid_df.groupby('mode')['MB/s'].agg(['mean', 'min', 'max'])\n",
    "        print(mode_perf)\n",
    "    \n",
    "    print(\"\\nLatency comparison:\")\n",
    "    mode_latency = hybrid_df.groupby('mode')['ns_op'].agg(['mean', 'min', 'max'])\n",
    "    print(mode_latency)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Performance by mode and operation\n",
    "    # Only plot if we have numeric size data\n",
    "    if hybrid_df['size_numeric'].notna().any():\n",
    "        pivot = hybrid_df.pivot_table(index='size_numeric', columns=['mode', 'operation'], values='ns_op')\n",
    "        if not pivot.empty and pivot.shape[1] > 0:\n",
    "            pivot.plot(ax=ax1, marker='o', linewidth=2)\n",
    "            ax1.set_xlabel('Size', fontsize=12)\n",
    "            ax1.set_ylabel('Time (ns/op)', fontsize=12)\n",
    "            ax1.set_title('Hybrid Mode Performance', fontsize=14, fontweight='bold')\n",
    "            ax1.set_xscale('log')\n",
    "            ax1.legend(loc='best')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "        else:\n",
    "            ax1.text(0.5, 0.5, 'Insufficient data for pivot plot', \n",
    "                    ha='center', va='center', transform=ax1.transAxes)\n",
    "    else:\n",
    "        # Alternative: bar plot by benchmark name\n",
    "        plot_df = hybrid_df.sort_values('ns_op')\n",
    "        x_pos = np.arange(len(plot_df))\n",
    "        colors = ['skyblue' if 'Array' in name else 'coral' for name in plot_df['name']]\n",
    "        ax1.barh(x_pos, plot_df['ns_op'], color=colors, alpha=0.7)\n",
    "        ax1.set_yticks(x_pos)\n",
    "        ax1.set_yticklabels(plot_df['name'].str.replace('BenchmarkHybridModes/', ''), fontsize=8)\n",
    "        ax1.set_xlabel('Time (ns/op)', fontsize=12)\n",
    "        ax1.set_title('Hybrid Mode Performance', fontsize=14, fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Plot 2: Memory allocations by mode\n",
    "    mode_mem = hybrid_df.groupby('mode')['bytes_op'].mean()\n",
    "    if not mode_mem.empty and mode_mem.sum() > 0:\n",
    "        mode_mem.plot(kind='bar', ax=ax2, color=['skyblue', 'coral'], alpha=0.7)\n",
    "        ax2.set_ylabel('Average Bytes/op', fontsize=12)\n",
    "        ax2.set_title('Memory Usage by Mode', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Mode', fontsize=12)\n",
    "        ax2.tick_params(axis='x', rotation=0)\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "    else:\n",
    "        # If no memory allocations, show performance comparison instead\n",
    "        mode_perf = hybrid_df.groupby('mode')['ns_op'].mean()\n",
    "        mode_perf.plot(kind='bar', ax=ax2, color=['skyblue', 'coral'], alpha=0.7)\n",
    "        ax2.set_ylabel('Average Time (ns/op)', fontsize=12)\n",
    "        ax2.set_title('Average Performance by Mode', fontsize=14, fontweight='bold')\n",
    "        ax2.set_xlabel('Mode', fontsize=12)\n",
    "        ax2.tick_params(axis='x', rotation=0)\n",
    "        ax2.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    graph_path = os.path.join(output_dir, 'graphs', 'hybrid_mode_comparison.png')\n",
    "    plt.savefig(graph_path, dpi=300, bbox_inches='tight')\n",
    "    print(f\"\\n✓ Graph saved: {graph_path}\")\n",
    "    \n",
    "    svg_path = os.path.join(output_dir, 'graphs', 'hybrid_mode_comparison.svg')\n",
    "    plt.savefig(svg_path, format='svg', bbox_inches='tight')\n",
    "    print(f\"✓ Graph saved (SVG): {svg_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hybrid mode benchmarks found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Performers and Bottlenecks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TOP 10 FASTEST BENCHMARKS\")\n",
    "print(\"=\" * 80)\n",
    "display(df.nsmallest(10, 'ns_op')[['name', 'ns_op', 'ops_per_sec', 'bytes_op', 'allocs_op']])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TOP 10 SLOWEST BENCHMARKS\")\n",
    "print(\"=\" * 80)\n",
    "display(df.nlargest(10, 'ns_op')[['name', 'ns_op', 'ops_per_sec', 'bytes_op', 'allocs_op']])\n",
    "\n",
    "if 'MB/s' in df.columns:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"HIGHEST THROUGHPUT BENCHMARKS\")\n",
    "    print(\"=\" * 80)\n",
    "    high_throughput = df[df['MB/s'].notna()].nlargest(10, 'MB/s')\n",
    "    display(high_throughput[['name', 'MB/s', 'ns_op', 'bytes_op', 'allocs_op']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Results & Profile Analysis\n",
    "\n",
    "Export all benchmark data, CPU profiles, and generate flamegraphs for the complete investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all data to CSV files\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPORTING RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Export CSV files\n",
    "csv_file = os.path.join(output_dir, 'data', 'all_benchmarks.csv')\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"\\n✓ All benchmarks exported to: {csv_file}\")\n",
    "\n",
    "# Export SIMD comparison if available\n",
    "if not simd_df.empty and 'speedup' in comparison.columns:\n",
    "    simd_csv = os.path.join(output_dir, 'data', 'simd_comparison.csv')\n",
    "    comparison.to_csv(simd_csv, index=False)\n",
    "    print(f\"✓ SIMD comparison exported to: {simd_csv}\")\n",
    "    \n",
    "    # Export summary statistics\n",
    "    summary_data = {\n",
    "        'metric': ['Average Speedup', 'Best Speedup', 'Worst Speedup', 'Median Speedup'],\n",
    "        'value': [\n",
    "            f\"{comparison['speedup'].mean():.2f}x\",\n",
    "            f\"{comparison['speedup'].max():.2f}x\",\n",
    "            f\"{comparison['speedup'].min():.2f}x\",\n",
    "            f\"{comparison['speedup'].median():.2f}x\"\n",
    "        ]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_csv = os.path.join(output_dir, 'data', 'simd_summary.csv')\n",
    "    summary_df.to_csv(summary_csv, index=False)\n",
    "    print(f\"✓ SIMD summary statistics exported to: {summary_csv}\")\n",
    "\n",
    "# Export hybrid mode analysis if available\n",
    "if not hybrid_df.empty:\n",
    "    hybrid_csv = os.path.join(output_dir, 'data', 'hybrid_modes.csv')\n",
    "    hybrid_df.to_csv(hybrid_csv, index=False)\n",
    "    print(f\"✓ Hybrid mode data exported to: {hybrid_csv}\")\n",
    "    \n",
    "    # Mode comparison summary\n",
    "    mode_comparison = pd.DataFrame({\n",
    "        'mode': ['Array', 'Map'],\n",
    "        'avg_ns_op': [\n",
    "            hybrid_df[hybrid_df['mode'] == 'Array']['ns_op'].mean(),\n",
    "            hybrid_df[hybrid_df['mode'] == 'Map']['ns_op'].mean()\n",
    "        ],\n",
    "        'avg_bytes_op': [\n",
    "            hybrid_df[hybrid_df['mode'] == 'Array']['bytes_op'].mean(),\n",
    "            hybrid_df[hybrid_df['mode'] == 'Map']['bytes_op'].mean()\n",
    "        ]\n",
    "    })\n",
    "    mode_csv = os.path.join(output_dir, 'data', 'mode_comparison.csv')\n",
    "    mode_comparison.to_csv(mode_csv, index=False)\n",
    "    print(f\"✓ Mode comparison summary exported to: {mode_csv}\")\n",
    "\n",
    "# Export top performers\n",
    "top_10_fastest = df.nsmallest(10, 'ns_op')[['name', 'ns_op', 'ops_per_sec', 'bytes_op', 'allocs_op']]\n",
    "top_csv = os.path.join(output_dir, 'data', 'top_10_fastest.csv')\n",
    "top_10_fastest.to_csv(top_csv, index=False)\n",
    "print(f\"✓ Top 10 fastest benchmarks exported to: {top_csv}\")\n",
    "\n",
    "# Export top 10 slowest\n",
    "top_10_slowest = df.nlargest(10, 'ns_op')[['name', 'ns_op', 'ops_per_sec', 'bytes_op', 'allocs_op']]\n",
    "slow_csv = os.path.join(output_dir, 'data', 'top_10_slowest.csv')\n",
    "top_10_slowest.to_csv(slow_csv, index=False)\n",
    "print(f\"✓ Top 10 slowest benchmarks exported to: {slow_csv}\")\n",
    "\n",
    "# Export metadata\n",
    "metadata = {\n",
    "    'analysis_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'source_file': BENCHMARK_FILE,\n",
    "    'total_benchmarks': len(df),\n",
    "    'categories': len(df['category'].unique()),\n",
    "    'fastest_ns_op': df['ns_op'].min(),\n",
    "    'slowest_ns_op': df['ns_op'].max(),\n",
    "    'zero_alloc_count': len(df[df['allocs_op'] == 0]),\n",
    "    'with_alloc_count': len(df[df['allocs_op'] > 0])\n",
    "}\n",
    "metadata_df = pd.DataFrame([metadata])\n",
    "meta_csv = os.path.join(output_dir, 'data', 'metadata.csv')\n",
    "metadata_df.to_csv(meta_csv, index=False)\n",
    "print(f\"✓ Metadata exported to: {meta_csv}\")\n",
    "\n",
    "print(f\"\\n✓ All data files saved to: {os.path.join(output_dir, 'data')}\")\n",
    "print(f\"✓ All graphs have been saved to: {os.path.join(output_dir, 'graphs')}\")\n",
    "\n",
    "# Initialize profile variables\n",
    "prof_df = None\n",
    "prof_metadata = None\n",
    "func_df = None\n",
    "calls_df = None\n",
    "profile_metadata = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Comprehensive HTML Report\n",
    "\n",
    "Create a standalone HTML report with all visualizations and data tables embedded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import re\n",
    "\n",
    "def analyze_pprof_file(prof_file):\n",
    "    \"\"\"\n",
    "    Analyze a Go pprof CPU profile file.\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with profile data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Run go tool pprof to get top functions\n",
    "        result = subprocess.run(\n",
    "            ['go', 'tool', 'pprof', '-top', '-nodecount=50', prof_file],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=30\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"Error running pprof: {result.stderr}\")\n",
    "            return None, None\n",
    "        \n",
    "        output = result.stdout\n",
    "        \n",
    "        # Extract metadata\n",
    "        metadata = {}\n",
    "        for line in output.split('\\n')[:10]:\n",
    "            if 'Duration:' in line:\n",
    "                metadata['duration'] = line.split('Duration:')[1].split(',')[0].strip()\n",
    "                metadata['total_samples'] = line.split('Total samples =')[1].split('(')[0].strip()\n",
    "            elif 'Type:' in line:\n",
    "                metadata['type'] = line.split('Type:')[1].strip()\n",
    "            elif 'Time:' in line:\n",
    "                metadata['time'] = line.split('Time:')[1].strip()\n",
    "        \n",
    "        # Parse the profile data\n",
    "        profile_data = []\n",
    "        in_data_section = False\n",
    "        \n",
    "        for line in output.split('\\n'):\n",
    "            # Start parsing after the header\n",
    "            if 'flat  flat%   sum%' in line:\n",
    "                in_data_section = True\n",
    "                continue\n",
    "            \n",
    "            if not in_data_section:\n",
    "                continue\n",
    "            \n",
    "            # Parse data lines\n",
    "            # Format: flat  flat%   sum%        cum   cum%  function_name\n",
    "            match = re.match(r'\\s*(\\d+\\.?\\d*[a-z]*)\\s+(\\d+\\.?\\d+)%\\s+(\\d+\\.?\\d+)%\\s+(\\d+\\.?\\d*[a-z]*)\\s+(\\d+\\.?\\d+)%\\s+(.+)', line)\n",
    "            if match:\n",
    "                profile_data.append({\n",
    "                    'flat': match.group(1),\n",
    "                    'flat_pct': float(match.group(2)),\n",
    "                    'sum_pct': float(match.group(3)),\n",
    "                    'cum': match.group(4),\n",
    "                    'cum_pct': float(match.group(5)),\n",
    "                    'function': match.group(6).strip()\n",
    "                })\n",
    "        \n",
    "        if not profile_data:\n",
    "            print(\"No profile data found\")\n",
    "            return None, None\n",
    "        \n",
    "        df = pd.DataFrame(profile_data)\n",
    "        \n",
    "        # Simplify function names for display\n",
    "        df['function_short'] = df['function'].apply(lambda x: x.split('/')[-1] if '/' in x else x)\n",
    "        df['function_short'] = df['function_short'].apply(lambda x: x[:60] + '...' if len(x) > 60 else x)\n",
    "        \n",
    "        return df, metadata\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"pprof analysis timed out\")\n",
    "        return None, None\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: 'go' command not found. Make sure Go is installed and in PATH.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing pprof file: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CPU PROFILE ANALYSIS (.prof files)\n",
    "# =============================================================================\n",
    "if PROFILE_FILE and os.path.exists(PROFILE_FILE):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"CPU PROFILE ANALYSIS: {os.path.basename(PROFILE_FILE)}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    prof_df, prof_metadata = analyze_pprof_file(PROFILE_FILE)\n",
    "    \n",
    "    if prof_df is not None:\n",
    "        print(f\"\\nProfile Metadata:\")\n",
    "        for key, value in prof_metadata.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "        \n",
    "        print(f\"\\n✓ Parsed {len(prof_df)} functions from profile\")\n",
    "        print(f\"\\nTop 10 CPU Consumers:\")\n",
    "        display(prof_df.head(10)[['function_short', 'flat_pct', 'cum_pct']])\n",
    "        \n",
    "        # Visualize top functions\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Plot 1: Top 15 functions by flat %\n",
    "        top_flat = prof_df.head(15)\n",
    "        ax1.barh(range(len(top_flat)), top_flat['flat_pct'], color='steelblue', alpha=0.7)\n",
    "        ax1.set_yticks(range(len(top_flat)))\n",
    "        ax1.set_yticklabels(top_flat['function_short'], fontsize=9)\n",
    "        ax1.set_xlabel('Flat % (self time)', fontsize=12)\n",
    "        ax1.set_title('Top 15 Functions by Self Time', fontsize=14, fontweight='bold')\n",
    "        ax1.grid(True, alpha=0.3, axis='x')\n",
    "        ax1.invert_yaxis()\n",
    "        \n",
    "        # Plot 2: Top 15 functions by cumulative %\n",
    "        top_cum = prof_df.nlargest(15, 'cum_pct')\n",
    "        ax2.barh(range(len(top_cum)), top_cum['cum_pct'], color='coral', alpha=0.7)\n",
    "        ax2.set_yticks(range(len(top_cum)))\n",
    "        ax2.set_yticklabels(top_cum['function_short'], fontsize=9)\n",
    "        ax2.set_xlabel('Cumulative % (including callees)', fontsize=12)\n",
    "        ax2.set_title('Top 15 Functions by Cumulative Time', fontsize=14, fontweight='bold')\n",
    "        ax2.grid(True, alpha=0.3, axis='x')\n",
    "        ax2.invert_yaxis()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the figure\n",
    "        graph_path = os.path.join(output_dir, 'graphs', 'cpu_profile_analysis.png')\n",
    "        plt.savefig(graph_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"\\n✓ Graph saved: {graph_path}\")\n",
    "        \n",
    "        svg_path = os.path.join(output_dir, 'graphs', 'cpu_profile_analysis.svg')\n",
    "        plt.savefig(svg_path, format='svg', bbox_inches='tight')\n",
    "        print(f\"✓ Graph saved (SVG): {svg_path}\")\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Export profile data\n",
    "        prof_csv = os.path.join(output_dir, 'data', 'cpu_profile.csv')\n",
    "        prof_df.to_csv(prof_csv, index=False)\n",
    "        print(f\"✓ CPU profile data exported to: {prof_csv}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CPU PROFILE ANALYSIS SKIPPED\")\n",
    "    print(\"=\" * 80)\n",
    "    if PROFILE_FILE is None:\n",
    "        print(\"\\nNo CPU profile file specified for this investigation.\")\n",
    "    print(\"\\nTo generate CPU profiles, run:\")\n",
    "    print(\"  go test -bench=BenchmarkName -cpuprofile=results/cpu.prof\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nFor interactive profile visualization, use:\")\n",
    "print(\"  go tool pprof -http=:8080 results/cpu.prof\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}